{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1V3K0J72-Tfzro75cfCIS_zN4EMe7WiM0","timestamp":1746327682581}],"gpuType":"T4","authorship_tag":"ABX9TyM+m5CIJy+HCon5qYJRWzu8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!apt-get update\n","!apt-get install -y xvfb python3-opengl ffmpeg libosmesa6-dev libgl1-mesa-glx libglib2.0-0 patchelf\n","!pip install gymnasium mujoco==2.3.3 matplotlib pyvirtualdisplay imageio imageio-ffmpeg\n","\n","import os\n","os.environ['MUJOCO_GL'] = 'osmesa'\n","\n","from pyvirtualdisplay import Display\n","import gymnasium as gym\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","import imageio\n","import imageio.v3 as iio\n","\n","print(\"Starting virtual display...\")\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","time.sleep(5)\n","\n","env = None\n","env_name = None\n","try:\n","    env = gym.make('HalfCheetah-v2', render_mode='rgb_array')\n","    env_name = 'HalfCheetah-v2'\n","    print(\"Successfully created HalfCheetah-v2 environment\")\n","except:\n","    try:\n","        env = gym.make('HalfCheetah-v4', render_mode='rgb_array')\n","        env_name = 'HalfCheetah-v4'\n","        print(\"Successfully created HalfCheetah-v4 environment\")\n","    except:\n","        try:\n","            env = gym.make('Ant-v4', render_mode='rgb_array')\n","            env_name = 'Ant-v4'\n","            print(\"HalfCheetah not available, using Ant-v4 as fallback\")\n","        except:\n","            print(\"Ant-v4 not available, please check MuJoCo installation.\")\n","            exit()\n","\n","print(f\"Environment: {env_name}\")\n","print(f\"Action space: {env.action_space}\")\n","print(f\"Observation space: {env.observation_space}\")\n","\n","action_bins = 2\n","state_bins = 2\n","action_dim = env.action_space.shape[0]\n","state_dim = env.observation_space.shape[0]\n","action_max = env.action_space.high\n","action_min = env.action_space.low\n","state_clip = 25.0\n","\n","sarsa_table = {}\n","\n","def discretize_state(state):\n","    \"\"\"Discretize continuous state into 2 bins, clipped to [-25, 25].\"\"\"\n","    state = np.clip(state, -state_clip, state_clip)\n","    discrete_state = []\n","    for i in range(state_dim):\n","        bin_idx = int((state[i] + state_clip) / (2 * state_clip) * state_bins)\n","        discrete_state.append(min(bin_idx, state_bins-1))\n","    return tuple(discrete_state)\n","\n","def discretize_action(action):\n","    \"\"\"Discretize continuous action into 2 bins.\"\"\"\n","    discrete_action = []\n","    for i in range(action_dim):\n","        bin_idx = int((action[i] - action_min[i]) / (action_max[i] - action_min[i]) * action_bins)\n","        discrete_action.append(min(bin_idx, action_bins-1))\n","    return tuple(discrete_action)\n","\n","def get_continuous_action(discrete_action):\n","    \"\"\"Convert discrete action to continuous action.\"\"\"\n","    continuous_action = np.zeros(action_dim)\n","    for i in range(action_dim):\n","        bin_val = discrete_action[i]\n","        continuous_action[i] = action_min[i] + (bin_val + 0.5) * (action_max[i] - action_min[i]) / action_bins\n","    return continuous_action\n","\n","def choose_action(discrete_state, epsilon):\n","    \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n","    if random.uniform(0, 1) < epsilon:\n","        return tuple(random.randint(0, action_bins-1) for _ in range(action_dim))\n","    else:\n","        if discrete_state in sarsa_table and sarsa_table[discrete_state]:\n","            return max(sarsa_table[discrete_state].items(), key=lambda x: x[1])[0]\n","        else:\n","            return tuple(random.randint(0, action_bins-1) for _ in range(action_dim))\n","\n","def record_video(env, sarsa_table, filename=\"sarsa_video.mp4\", num_frames=1000):\n","    \"\"\"Record a video of the agent acting in the environment using the SARSA table.\"\"\"\n","    frames = []\n","    state, _ = env.reset()\n","    for i in range(num_frames):\n","        frame = env.render()\n","        frames.append(frame)\n","\n","        discrete_state = discretize_state(state)\n","        discrete_action = choose_action(discrete_state, epsilon=0.0)\n","        continuous_action = get_continuous_action(discrete_action)\n","        state, reward, done, truncated, _ = env.step(continuous_action)\n","\n","        if done or truncated:\n","            state, _ = env.reset()\n","\n","    iio.imwrite(filename, frames, fps=30, codec='h264')\n","    print(f\"Video saved as {filename}\")\n","\n","def record_random_video(env, filename=\"random_action_video.mp4\", num_frames=100):\n","    \"\"\"Record a video with random actions to verify setup.\"\"\"\n","    frames = []\n","    state, _ = env.reset()\n","    for i in range(num_frames):\n","        frame = env.render()\n","        frames.append(frame)\n","        action = env.action_space.sample()\n","        state, reward, done, truncated, _ = env.step(action)\n","        if done or truncated:\n","            state, _ = env.reset()\n","\n","    iio.imwrite(filename, frames, fps=30, codec='h264')\n","    print(f\"Random action video saved as {filename}\")\n","\n","print(\"Recording a test video with random actions...\")\n","try:\n","    record_random_video(env)\n","    print(\"Test video recorded successfully!\")\n","except Exception as e:\n","    print(f\"Error recording test video: {e}\")\n","\n","def train_sarsa(env, num_episodes=500, alpha=0.5, gamma=0.99, epsilon_start=0.99):\n","    \"\"\"Train SARSA agent with paper-aligned parameters and video recording.\"\"\"\n","    global sarsa_table\n","    rewards = []\n","    avg_rewards = []\n","    epsilon = epsilon_start\n","    start_time = time.time()\n","\n","    for episode in range(num_episodes):\n","        state, _ = env.reset()\n","        discrete_state = discretize_state(state)\n","\n","        discrete_action = choose_action(discrete_state, epsilon)\n","\n","        total_reward = 0\n","        done = False\n","        truncated = False\n","        steps = 0\n","\n","        while not (done or truncated) and steps < 1000:\n","            continuous_action = get_continuous_action(discrete_action)\n","            next_state, reward, done, truncated, _ = env.step(continuous_action)\n","            discrete_next_state = discretize_state(next_state)\n","\n","            discrete_next_action = choose_action(discrete_next_state, epsilon)\n","\n","            if discrete_state not in sarsa_table:\n","                sarsa_table[discrete_state] = {}\n","            if discrete_action not in sarsa_table[discrete_state]:\n","                sarsa_table[discrete_state][discrete_action] = 0\n","\n","            next_q = 0\n","            if not (done or truncated):\n","                if discrete_next_state not in sarsa_table:\n","                    sarsa_table[discrete_next_state] = {}\n","                if discrete_next_action not in sarsa_table[discrete_next_state]:\n","                    sarsa_table[discrete_next_state][discrete_next_action] = 0\n","                next_q = sarsa_table[discrete_next_state][discrete_next_action]\n","\n","            sarsa_table[discrete_state][discrete_action] = (\n","                sarsa_table[discrete_state][discrete_action] +\n","                alpha * (reward + gamma * next_q - sarsa_table[discrete_state][discrete_action])\n","            )\n","\n","            discrete_state = discrete_next_state\n","            discrete_action = discrete_next_action\n","\n","            total_reward += reward\n","            steps += 1\n","\n","            if episode == 0 and steps == 1:\n","                print(f\"Sample discrete state: {discrete_state}\")\n","\n","        rewards.append(total_reward)\n","        avg_rewards.append(total_reward if len(rewards) < 10 else sum(rewards[-10:]) / 10)\n","\n","        epsilon = math.log10((episode + 1) / 25)\n","        epsilon = max(0.1, min(epsilon, epsilon_start))\n","\n","        if (episode + 1) % 100 == 0:\n","            record_video(env, sarsa_table, f\"sarsa_video_episode_{episode + 1}.mp4\")\n","\n","        if (episode + 1) % 10 == 0 or episode == 0:\n","            elapsed = time.time() - start_time\n","            print(f\"Episode {episode + 1}/{num_episodes}, Reward: {total_reward:.2f}, \"\n","                  f\"Avg(10): {avg_rewards[-1]:.2f}, Epsilon: {epsilon:.2f}, \"\n","                  f\"Steps: {steps}, SARSA table size: {len(sarsa_table)}, Time: {elapsed:.1f}s\")\n","\n","    record_video(env, sarsa_table, \"final_sarsa_video.mp4\")\n","    return rewards, avg_rewards\n","\n","learning_rates = [0.2, 0.5, 0.9]\n","all_rewards = {}\n","all_avg_rewards = {}\n","\n","for alpha in learning_rates:\n","    print(f\"\\nStarting SARSA training with alpha={alpha}...\")\n","    sarsa_table = {}\n","    rewards, avg_rewards = train_sarsa(\n","        env,\n","        num_episodes=500,\n","        alpha=alpha,\n","        gamma=0.99,\n","        epsilon_start=0.99\n","    )\n","    all_rewards[alpha] = rewards\n","    all_avg_rewards[alpha] = avg_rewards\n","\n","plt.figure(figsize=(12, 6))\n","for alpha in learning_rates:\n","    plt.plot(all_avg_rewards[alpha], label=f'alpha={alpha}')\n","plt.title(f'SARSA - Average Reward (10 episodes) for {env_name}')\n","plt.xlabel('Episode')\n","plt.ylabel('Average Reward')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"\\nEvaluating the trained SARSA agent (alpha=0.5) over 10 episodes for {env_name}...\")\n","eval_env = gym.make(env_name, render_mode='rgb_array')\n","eval_rewards = []\n","\n","for i in range(10):\n","    state, _ = eval_env.reset()\n","    total_reward = 0\n","    done = False\n","    truncated = False\n","    steps = 0\n","\n","    if i == 0:\n","        frames = []\n","\n","    while not (done or truncated) and steps < 1000:\n","        if i == 0:\n","            frame = eval_env.render()\n","            frames.append(frame)\n","\n","        discrete_state = discretize_state(state)\n","        discrete_action = choose_action(discrete_state, epsilon=0.0)\n","        continuous_action = get_continuous_action(discrete_action)\n","        state, reward, done, truncated, _ = eval_env.step(continuous_action)\n","        total_reward += reward\n","        steps += 1\n","\n","    eval_rewards.append(total_reward)\n","    print(f\"Evaluation episode {i+1}: Reward = {total_reward:.2f}, Steps = {steps}\")\n","\n","    if i == 0:\n","        iio.imwrite(\"evaluation_sarsa_video.mp4\", frames, fps=30, codec='h264')\n","        print(\"Evaluation video saved as evaluation_sarsa_video.mp4\")\n","\n","print(f\"\\nEvaluation complete! Average reward over 10 episodes: {sum(eval_rewards) / len(eval_rewards):.2f}\")\n","print(f\"SARSA table final size: {len(sarsa_table)} states\")\n","eval_env.close()\n","\n","print(\"\\nSARSA Learning Statistics:\")\n","print(f\"- Environment: {env_name}\")\n","print(f\"- State dimensions: {state_dim}, clipped to [-25, 25]\")\n","print(f\"- Action space dimensions: {action_dim}\")\n","print(f\"- Discretization bins per state dimension: {state_bins}\")\n","print(f\"- Discretization bins per action dimension: {action_bins}\")\n","print(f\"- Final SARSA table size: {len(sarsa_table)} states\")\n","print(f\"- Average reward in final 10 training episodes (alpha=0.5): {all_avg_rewards[0.5][-1]:.2f}\")\n","\n","print(\"\\nExample SARSA table entries:\")\n","count = 0\n","for state, actions in sarsa_table.items():\n","    if count >= 3:\n","        break\n","    if actions:\n","        best_action = max(actions.items(), key=lambda x: x[1])[0]\n","        best_value = actions[best_action]\n","        print(f\"State {state}: Best action {best_action}, Value {best_value:.2f}\")\n","        count += 1\n","\n","print(\"Stopping virtual display...\")\n","display.stop()"],"metadata":{"id":"cwphn3Mccy93"},"execution_count":null,"outputs":[]}]}